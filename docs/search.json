[
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "C.V.",
    "section": "",
    "text": "For your convenience, you can download a PDF copy of my CV here."
  },
  {
    "objectID": "poetry/index.html",
    "href": "poetry/index.html",
    "title": "Poetry",
    "section": "",
    "text": "I also write poetry. Some of them have even been published! You can browse those works here."
  },
  {
    "objectID": "poetry/index.html#forthcoming",
    "href": "poetry/index.html#forthcoming",
    "title": "Poetry",
    "section": "Forthcoming",
    "text": "Forthcoming\n\n\n\n\n  \n    \n      For Grace, After A Wedding\n      \n      \n        \n        \n          Yearling. Expected December 2025. (after Frank O'Hara).\n        \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      nine thousand nine hundred ninety nine days\n      \n      \n        \n        \n          LEON Literary Review. Expected November 2025.\n        \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      In my hometown\n      \n      \n        \n        \n          Eulogy Press. Expected July 2025.\n        \n      \n      \n      \n      \n      \n      \n    \n  \n\nNo matching items"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Zachary Lorico Hertz",
    "section": "",
    "text": "Hey, it’s good to see you here!\nThanks for taking the time to get to know me. My name is Zachary Lorico Hertz and I am a third-year Ph.D. student at UC Berkeley. I study how identities develop, are expressed through political behavior, and intersect with power in local politics. In my research, I use causal inference methods, geocoded large-N observational data, and original survey data to study the effects an increasingly diverse and partisan electorate will have on representation and sub-national institutions. My writing has been featured in academic and popular outlets, including the Election Law Journal and the Washington Post.\nCurrently, I am the Programming and Data Manager at the Berkeley IGS Poll. Previously, I worked as an Analyst at Data for Progress, where I fielded and analyzed polling data for a number of clients in progressive politics. In addition to my academic research, I have done independent political consulting for Groundwork Project, Data for Progress, and private clients. I received an M.A. from the University of Chicago, and a B.A. from Tufts University. I also write poetry, with work forthcoming in Yearling, LEON Literary Magazine, and Eulogy Press.\nWhile much of my time is devoted to reading, writing, or coding, I enjoy hiking, quiz bowl, and both performing and producing music in my spare time. I am in a band with other Ph.D. students called Good Bones, named for the poem by Maggie Smith (no, not that one). Send me an email and let’s talk more!"
  },
  {
    "objectID": "posts/2021-11-23-educational-polarization/index.html",
    "href": "posts/2021-11-23-educational-polarization/index.html",
    "title": "Educational Polarization: A White Phenomenon?",
    "section": "",
    "text": "Recently, CES Researcher Pia Deshpande wrote an excellent tutorial detailing how to plot trends over time using CES data — I highly encourage anyone who hasn’t yet read the piece to promptly do so! This style of plot pairs particularly well with the cumulative CES Common Content dataset created by Shiro Kuriwaki. Together, these resources can help us to understand an issue that has received increased attention in recent months: educational polarization.\nResearch has noted an increasing divide in party identification along educational lines. But less attention has been devoted to how these changes differ by race. Using CES data, I note that across most racial groups, Americans with a college education less strongly identify as Republicans while Americans without a college degree less strongly identify as Democrats. These trends in partisan identification are complex, however, and differ in strength and effect by race. When it comes to vote choice, however, educational polarization is mostly limited to white Americans."
  },
  {
    "objectID": "posts/2021-11-23-educational-polarization/index.html#footnotes",
    "href": "posts/2021-11-23-educational-polarization/index.html#footnotes",
    "title": "Educational Polarization: A White Phenomenon?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt is worth noting that Black Americans with a college degree were the only college-educated racial group to identify as Democrats at a statistically significant lower rate in 2020 than in 2016 and 2012; this decline, however, was less pronounced than the attrition among those without a college degree so still led to a statistically significant educational gap.↩︎\nLike before, the relatively smaller sample size of Asian Americans in the CES leads to large errors for the point estimates, so it is an important caveat to note that the educational differences in 2008 and 2012 are not statistically significant. Still, following the overall trends these differences grow and by 2016 and 2020 the difference in the point estimates are statistically significant.↩︎"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Truth Hertz",
    "section": "",
    "text": "This blog tracks a number of short data-driven posts I have written over the years analyzing survey data, methodology, and electoral trends. I may also throw other musings here when I have the time to neglect my research.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Can Survey Data Tell Us About Ideological Differences Between Black Voters and Black Nonvoters?\n\n\nHow much do Black 2020 voters differ from Black 2020 nonvoters, and what does this mean for the 2024 election?\n\n\n\nelectoral analysis\n\nR\n\nCES\n\nBlack Americans\n\nturnout\n\nrace and ethnicity\n\n\n\n\n\n\nNov 5, 2024\n\n\nZachary Lorico Hertz\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Survey Weights in R Using Census Data\n\n\nBoy, you’re gonna cre-ate that weight: A quick guide to creating original survey weights using Census data in R.\n\n\n\nR\n\ntutorial\n\nsurvey\n\ntidyverse\n\nsurvey data\n\nACS\n\nCensus\n\n\n\n\n\n\nMay 16, 2022\n\n\nZachary Lorico Hertz\n\n\n\n\n\n\n\n\n\n\n\n\nHow Sensitive are Partisans to Out-Party Cues?\n\n\nAn experiment testing if out-party cues still move policy preferences.\n\n\n\nR\n\nparty cues\n\nelite cues\n\npartisan polarization\n\npolicy learning\n\n\n\n\n\n\nDec 16, 2021\n\n\nZachary Lorico Hertz\n\n\n\n\n\n\n\n\n\n\n\n\nEducational Polarization: A White Phenomenon?\n\n\nEducational polarization may not be limited to white voters.\n\n\n\nelectoral analysis\n\nR\n\nCES\n\neducational polarization\n\nracial polariation\n\ncollege education\n\n\n\n\n\n\nNov 23, 2021\n\n\nZachary Lorico Hertz\n\n\n\n\n\n\n\n\n\n\n\n\nEstimates of Youth Turnout Have Recently Diverged\n\n\nUsing multiple data sources to investigate age-based turnout rates.\n\n\n\nR\n\nCES\n\nCIRCLE\n\nyouth voting\n\nvoter turnout\n\n\n\n\n\n\nAug 6, 2021\n\n\nZachary Lorico Hertz\n\n\n\n\n\n\n\n\n\n\n\n\nTracking the Drop in News Interest\n\n\nData shows a decline in news interest is consistent across party lines, not partisan\n\n\n\nR\n\nvoter analysis\n\nnews interest\n\nmedia attention\n\nsurvey data\n\n\n\n\n\n\nJul 13, 2021\n\n\nZachary Lorico Hertz\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the CES to Examine Young Voters\n\n\nHave age-based turnout gaps persisted?\n\n\n\nelectoral analysis\n\nR\n\nCES\n\nCIRCLE\n\nyouth voting\n\nvoter turnout\n\n\n\n\n\n\nJul 6, 2021\n\n\nZachary Lorico Hertz\n\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to Using the {survey} Package in R\n\n\nA quick guide to implementing and using survey weights in R.\n\n\n\nR\n\ntutorial\n\nsurvey\n\ntidyverse\n\nsurvey data\n\n\n\n\n\n\nJun 29, 2021\n\n\nZachary Lorico Hertz\n\n\n\n\n\n\n\n\n\n\n\n\nInvestigating Concerns about Lucid Theorem Data Quality\n\n\nHas Lucid Theorem declined in quality due to Covid-19?\n\n\n\nsurvey methodology\n\nData For Progress\n\nLucid\n\n\n\n\n\n\nJun 22, 2021\n\n\nZachary Lorico Hertz\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-05-16-survey-weights/index.html",
    "href": "posts/2022-05-16-survey-weights/index.html",
    "title": "Creating Survey Weights in R Using Census Data",
    "section": "",
    "text": "In survey research, the composition of a sample may differ notably from the population being modeled across important characteristics (e.g. race, age, education, party identification). These sampling errors often reflect systematic bias which can pose a threat to accuracy and the researcher’s ability to make inferences using the data, especially if the error is correlated with the variables of interest.\nIn response, researchers often construct “post-stratification weights” — values assigned to each observation which can be used to correct for sampling error and match the sample to the population on key characteristics. The idea is simple: by increasing the influence of under-represented units and likewise decreasing the influence of over-represented units, researchers can create a more representative sample. This is particularly important given the dramatic increase in the use of online opt-in (or “nonprobability”) samples.\nAn important implication is that you can only construct post-stratification weights using characteristics that are both measured in your data and known for the population; put more simply, if we want to make our sample reflect the population, we have to know what the population looks like! Thus, the American Commmunity Survey (ACS), a premier data source with detailed population information, is an incredible resource to construct population targets for survey weighting.\nLast year, I created survey weights for a project fielded with the Tufts Public Opinion Lab, but the task proved to be fairly involved. First, accessing ACS data requires navigating the Census Bureau’s fairly labyrinthine website. In addition, the size of the national ACS files – totaling over 10 gigabytes – is far beyond what my computer’s memory could possibly handle and crashed R Studio the first time I tried to read them in.\nIn this blog post, I create weights for a simplified version of the TPOL data. I walk through the process of accessing ACS data and using it to construct survey weights using R. I wrote this piece for myself, future lab students, and anyone interested in learning how to access ACS data and use it to create survey weights in R.\nThis post assumes familiarity with the basic concept of post-stratification weights and their uses; those in search of additional reading on weighting are directed to this handy piece on different weighting methods by Pew. While there are multiple weighting methods, the {anesrake} package allows for accessible and automated raking so I use this method to create the weights. To follow along, you will need a basic working knowledge of R. You’ll also use the {data.table}, {survey}, and {tidyverse} packages so be sure to install those by running install.packages() in the console as needed if you haven’t installed them already."
  },
  {
    "objectID": "posts/2022-05-16-survey-weights/index.html#a-quick-note-on-acs-time-periods",
    "href": "posts/2022-05-16-survey-weights/index.html#a-quick-note-on-acs-time-periods",
    "title": "Creating Survey Weights in R Using Census Data",
    "section": "A quick note on ACS time periods",
    "text": "A quick note on ACS time periods\nRemember, the ACS provides estimates for a specific given time period: one year, three years, or five years. It is critical to note that 1-year estimates are not calculated as an average of 12 monthly values (and similarly, the 5-year estimates are not calculated as the average of 60 monthly values, nor as the average of five individual 1-year estimates).\nInstead, the ACS collects survey data continuously, nearly every day of the year and then aggregates the results over the specific time period, spread evenly to avoid placing uneven weight on any given month or year within the period. For ACS 1-year data, this time period is the calendar year, so the 2019 ACS 1-year estimate covers January 2019 to December 2019. For ACS 5-year data, this time period is five calendar years, so the 2019 ACS 5-year estimate covers January 2015 to December 2019.\nThis creates a tradeoff for researchers choosing between 1-year and 5-year estimates. 1-year data is the most current, but 5-year data can generally be more reliable due to the larger sample size. If both estimates are available for your year in question, which one should you use?\nFor rapidly changing geographic areas, 1-year data are best as the current data is more likely to show yearly fluctuations, but are only available for geographic areas with at least 65,000 people. If you are hoping to illustrate a smooth trend, however, 5-year data may be best since the 5-year periods overlap. Above all, you must consistently use the same estimate so be sure to pick either 1-year estimates or 5-year estimates (or 3-year estimates, if applicable)1 and stick with them.\nFor this post, I will be using the 5-year data. Regardless of which period you choose, you should be in a directory filled with .zip files, following the general path:\nwww2.census.gov/programs-surveys/acs/data/pums/{YEAR}/{1-Year/5-Year}\nHow do you interpret these zip files, to pick the appropriate data? Their names are a construction using three important features of the data:\n{file format}_{record type}{state}.zip\n“File format” should take on two values: ‘unix’, denoting SAS datasets, and ‘csv’, denoting comma separated value files. “Record type” is either ‘h’, denoting housing files, or ‘p’, denoting person files. Finally, the file name includes the relevant two-letter state abbreviation code, with the abbreviation “us” denoting the nationwide data.\nI prefer to work with the .csv files. Because I am constructing weighting targets for the population of all American adults, I need the person record type for the “us” geography. So, I find “csv_pus.zip” and download the file. Once you download the file, unzip it (on Mac, you can right click and open with Archive Utility). You should be able to see a README .pdf file, and then your .csv data. If you are using the five-year 2019 U.S. data, you will see four .csv files (psam_pusa.csv, psam_pusb.csv, psam_pusc.csv and psam_pusd.csv). Move the folder to an appropriate working directory2 where you can access the data when you start constructing weights in R."
  },
  {
    "objectID": "posts/2022-05-16-survey-weights/index.html#footnotes",
    "href": "posts/2022-05-16-survey-weights/index.html#footnotes",
    "title": "Creating Survey Weights in R Using Census Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhile somewhat irrelevant to our purposes here, I wanted to note that ACS 3-year estimates have been discontinued and are only available for the 2005-2007, 2006-2008, 2007-2009, 2008-2010, 2009-2011, 2010-2012 and 2011-2013 periods.↩︎\nBeginners in R looking to review working directories can refer to the segment beginning at 04:49 in the R basics video I created for Tufts students.↩︎\nIf this sounds like gibberish to you, no worries! Maybe check out this overview of lists and vectors.↩︎\n[Photo by visuals on Unsplash.]↩︎"
  },
  {
    "objectID": "posts/2021-08-05-youth-turnout/index.html",
    "href": "posts/2021-08-05-youth-turnout/index.html",
    "title": "Estimates of Youth Turnout Have Recently Diverged",
    "section": "",
    "text": "With the recent release of the final 2020 Cooperative Election Study (CES) dataset, which includes vote validation, I returned to an earlier project examining youth voter turnout. In looking at voter turnout among adults under the age of 30, I noticed a pronounced gap between estimates from the Center for Information & Research on Civic Learning and Engagement (CIRCLE) and the CES.\n\nCES and CIRCLE estimates of youth voter turnout have been fairly close, but differed by 12 percentage points in 2020\nThere are three different ways to measure turnout using the vote validation variables in the CES. I calculated CES estimates of voter turnout using the first method, coding the unmatched as non-voters. I collected CIRCLE estimates of youth voter turnout for 2020 and 2016, 2018, 2014, 2012, and 2010 from press releases. I then calculated the absolute value of the difference between the CES and CIRCLE estimates, then plotted the results.\n\nWe see that the 2010 and 2012 youth voter turnout estimates were incredibly close, with a difference of just 0.2 and 0.1 percentage points respectively. These differences widened slightly between 2014 and 2018 but remained within about 3 percentage points: estimates differed by 3.3 percentage points in 2014, 1.0 percentage points in 2016, and 2.6 percentage points in 2018. In 2020, however, while the CES validated vote data estimated that voter turnout among adults ages 18-29 was 38 percent, CIRCLE found that 50 percent of adults between 18 and 29 turned out to vote, marking a difference of 12 percentage points.\n\n\nPotential causes\nThe 12-point difference in the CIRCLE and CES estimates of youth voter turnout may be the result of methodological differences. CIRCLE states that their estimates are based on voter file data from 41 states — Alaska, DC, Hawaii, Maryland, Mississippi, New Hampshire, North Dakota, Utah, Wisconsin, and Wyoming lack reliable vote history data by age. These states are not omitted from the CES data, however, so unobserved youth voting behavior in the 10 omitted states could lead to different estimates from the different sampling frame.\nThe CES data guide also notes vote validation matches are only made when there is a high level of confidence that respondents are assigned to the correct record. Therefore, CES records may lack vote validation due to incomplete or inaccurate information. It is also possible that there may be a systemic issue with incomplete or inaccurate voter file data, particularly among those under the age of 30.\n\n\n\n\nCitationBibTeX citation:@online{lorico_hertz2021,\n  author = {Lorico Hertz, Zachary},\n  title = {Estimates of {Youth} {Turnout} {Have} {Recently} {Diverged}},\n  date = {2021-08-06},\n  url = {index.qmd/posts/2021-08-05-youth-turnout/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLorico Hertz, Zachary. 2021. “Estimates of Youth Turnout Have\nRecently Diverged.” August 6, 2021. index.qmd/posts/2021-08-05-youth-turnout/."
  },
  {
    "objectID": "posts/2021-07-13-news-interest/index.html",
    "href": "posts/2021-07-13-news-interest/index.html",
    "title": "Tracking the Drop in News Interest",
    "section": "",
    "text": "As cable news viewership dropped over the first half of 2021, political commentators were quick to attribute this drop in news interest to the Biden administration. But the first half of 2021 has also seen the Covid-19 pandemic – one of the largest news stories in 2020 – rapidly drop in salience since January thanks to falling infection rates and the largely successful vaccine rollout. As a result, it remains difficult to determine what role the Biden administration and the decreased need to closely follow the pandemic have played as potential drivers of the drop in news interest. I used the Data for Progress Covid-19 Tracking Poll to investigate.\n\nShare of Americans who follow politics “most of the time” fell 15 percent consistently\nThe Data for Progress Covid-19 Tracking Poll, with 25 waves since April 2020, has included a number of questions tracking Americans’ opinions on politics and the pandemic. As part of the survey, respondents answer a question tracking general news engagement:\n\nSome people follow what’s going on in government and public affairs most of the time, whether there’s an election going on or not. Others aren’t as interested. Would you say you follow what’s going on in government and public affairs…\n\nMost of the time\nSome of the time\nOnly now and then\nHardly at all\n\n\nThe share of American adults who say they follow politics “most of the time” dropped from 41.8 percent in January to 35.5 percent in May, marking a 15 percent decline in the highest level of news engagement. In the same time period, the share of those who follow politics “some of the time” and “hardly at all” both increased 3 percentage points, while the share of those who follow politics “only now and then” remained relatively stable.\n\n\n\nShare of Americans who closely follow political news have declined 15 percent since January\n\n\nIf this drop in the share of highly engaged Americans is driven mainly by the effects of the Biden administration, we might expect to see the trend in news interest differ among partisan lines. The data does not reflect a political difference in changing news interest, however. Breaking the sample down by party identification, presidential vote, and strength of partisan identification, we see a consistent 15 percent decline in the highest level of news engagement between January and May.\n\n\n\nPolitical news interest by party identification\n\n\nDemocrats and Republicans had similar trends in the levels of their news interest between October 2020 and January 2021. But while the share of Republicans who followed political news “most of the time” immediately started decreasing from 47.3 percent in January 2021 to 39.8 percent in May 2021, the share of Democrats who were highly-engaged followers hovered around 47 percent until March 2021, but declined to 38 percent by May.\nWe might also expect that any observable effects of the Biden administration on news interest are conditional on having voted for or against his presidency. Unfortunately, the number of nonvoters in the sample is too small to draw reliable conclusions from, but we might imagine that news interest changed differently among Biden voters, who might become less engaged under an in-party administration, and Trump voters, who suddenly find themselves in the outparty. But while total levels of those most interested in following government affairs was higher among the voters than partisans as a whole, the share of respondents who follow government “most of the time” still dropped 15 percent among both Biden and Trump voters.\n\n\n\nPolitical news interest by presidential vote\n\n\nNotably, the share of those who say they follow government and public affairs “most of the time” rises through the late fall of 2020 among voters from both parties, a trend that is not visible when subsetting by party identification instead of presidential vote choice. But both Biden voters and Trump voters see significant declines of 15 percent in the share of respondents who follow the news most closely between January and May of 2021.\nAnother approach to investigate whether the drop in news interest differs by political engagement is to examine how strong and weak partisans differ in their news consumption patterns. Strong partisans might be expected to maintain higher engagement regardless of who is in power, while weak partisans could be more susceptible to administration-driven changes in interest.\n\n\n\nPolitical news interest by partisan strength\n\n\nThe data shows that both strong and weak partisans experienced similar declines in high-level news engagement after January 2021. Strong partisans, who maintained consistently higher rates of following politics “most of the time” throughout the period, still saw their engagement drop from around 55-60 percent to 45-50 percent. Weak partisans experienced a parallel decline from roughly 35 percent to 30 percent.\n\n\nNews source consumption patterns mirror overall engagement trends\nBeyond general political interest, the survey also tracked specific news source consumption, asking respondents whether they got news from various outlets in the past week. The results show broad declines across multiple news sources after January 2021.\n\n\n\nNews source consumption over time\n\n\nLocal news consumption shows the steepest decline, falling from around 60 percent in early 2020 to below 50 percent by early 2021, and continuing to decline through May. Cable news networks (CNN, Fox News, MSNBC) all experienced similar patterns, with viewership peaking during the election period and dropping significantly after inauguration. Even social media news consumption through Facebook declined notably.\nWhen broken down by partisan strength, the patterns remain consistent across both strong and weak partisans, suggesting that the decline in news source consumption is not primarily driven by partisan disengagement.\n\n\n\nNews source consumption by partisan strength\n\n\nSimilarly, examining news consumption by party identification and presidential vote choice reveals parallel declines across different political groups, reinforcing the finding that this trend transcends partisan divides.\n\n\n\nNews source consumption by party identification\n\n\n\n\n\nNews source consumption by presidential vote\n\n\n\n\nConclusion\nThe evidence strongly suggests that the decline in political news interest during the first half of 2021 was not primarily driven by partisan reactions to the Biden administration. Instead, the data points to a broader, non-partisan phenomenon affecting Americans across the political spectrum.\nSeveral key findings support this conclusion. The 15 percent decline in high-level political engagement was remarkably consistent across party lines, voter preferences, and levels of partisan strength. Republicans and Democrats, Biden voters and Trump voters, strong partisans and weak partisans all experienced similar drops in their likelihood to follow politics “most of the time.” This consistency suggests a common cause rather than partisan-specific reactions.\nThe timing also supports a pandemic-related explanation. The decline began precisely as COVID-19 cases were falling and vaccine distribution was accelerating, reducing the urgency that had driven news consumption throughout 2020. The pandemic had created an environment where staying informed about government actions felt critically important for personal safety and decision-making. As that immediate threat receded, so did the intense need to follow political developments.\nAdditionally, the 2020 election cycle had created an unusually high-engagement period that was unlikely to be sustainable. The combination of a contentious presidential election, a global pandemic, and significant social unrest had elevated political news consumption to extraordinary levels. Some decline from this peak was probably inevitable regardless of which candidate won the presidency.\nThis analysis suggests that commentators may have been too quick to attribute declining news interest to Biden-specific factors. While presidential administrations certainly influence the political news environment, the data indicates that broader contextual factors—particularly the winding down of the pandemic emergency—played a more significant role in the 2021 decline in political engagement than partisan reactions to the change in administration.\n\n\n\n\nCitationBibTeX citation:@online{lorico_hertz2021,\n  author = {Lorico Hertz, Zachary},\n  title = {Tracking the {Drop} in {News} {Interest}},\n  date = {2021-07-13},\n  url = {index.qmd/posts/2021-07-13-news-interest/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLorico Hertz, Zachary. 2021. “Tracking the Drop in News\nInterest.” July 13, 2021. index.qmd/posts/2021-07-13-news-interest/."
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "Use the filters on the right to browse my research by topic. If you’d prefer, you can download my CV in .pdf format here. You can also follow me on Google Scholar."
  },
  {
    "objectID": "research/index.html#published-papers",
    "href": "research/index.html#published-papers",
    "title": "Research",
    "section": "Published Papers",
    "text": "Published Papers\n\n\n\n\n\n  \n    \n      Does a Switch to By-District Elections Reduce Racial Turnout Disparities in Local Elections? The Impact of the California Voting Rights Act \n      \n      \n      \n      \n      \n      \n      \n        \n          2023. Zachary Lorico Hertz. Election Law Journal.\n        \n      \n      \n      \n      \n        \n          Abstract (click to expand)\n          \n            \n              The literature finds that an underrepresented group's comparative share of the population may moderate the effects of the California Voting Rights Act of 2001 on descriptive representation. Little attention has been devoted to the potential mechanisms driving these effects. Previous research suggests that electoral influence, conceptualized as an underrepresented group's relative size in a given political unit, can lead to an increase in turnout and subsequent descriptive representation. This article leverages ecological inference with nearest-neighbor matching and difference-in-differences methods to determine whether increased electoral influence following a switch from at-large to by-district elections as a result of the CVRA increased turnout among underrepresented groups. In my analysis, I find initial evidence suggesting that there is indeed a causal link between a CVRA-induced change in electoral institution and a reduction in the turnout gap. I do not find evidence to support my hypothesis that an increase in relative group size leads to a decrease in the turnout gap. I also do not find evidence to support my hypothesis that the effects of a switch to by-district elections on the turnout gap are more pronounced in cities where a minority group is a higher than average share of the total population. Instead, I find evidence that the treatment effects are more pronounced in cities where Hispanics are a lower than average share of the total population. In this work, I evaluate how the CVRA affects local California electorates, explain potential explanations for my findings and discuss potential areas for future research.\n            \n          \n        \n      \n\n      \n      \n      \n      \n      \n        \n            PDF \n        \n       \n      \n      \n      \n      \n        \n            Publisher's Version \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n\nNo matching items"
  },
  {
    "objectID": "research/index.html#working-papers",
    "href": "research/index.html#working-papers",
    "title": "Research",
    "section": "Working Papers",
    "text": "Working Papers\n\n\n\n\n\n  \n    \n      Noli nos tangere: Have Anti-Asian Hate Crimes Increased Asian-American Pan-Ethnicity, Participation, and Partisanship? \n      \n      \n      \n      \n        \n          May 15, 2025. Zachary Lorico Hertz. Working paper.\n        \n      \n      \n      \n      \n      \n      \n      \n        \n          Abstract (click to expand)\n          \n            \n              Hate crimes against Asian Americans in the United States spiked over 300 percent between 2020 and 2021. How might this troubling trend affect political participation and attitudes? Previous research finds that out-group threats might increase the salience of group-based identities and consequently shape political attitudes, group cohesion, and behavior (Huddy 2013), but reach mixed conclusions on whether these events are politically mobilizing or demobilizing. To investigate, I pair data encoding the geographic placement of hate crimes from 2010-2022 with individual-level turnout data to causally identify whether local hate crimes increased turnout among Asian Americans. To test potential mechanisms, I design an original survey experiment to estimate how the increased salience of hate crimes affects measures of Asian pan-ethnicity, stated vote likelihood, partisan identity and strength, and other behavioral outcomes. I find that hate crimes can strengthen group cohesion among the targeted group, but limited evidence that hate crimes have politically mobilized the targeted group and null effects on partisanship. I conclude that identity is a more complex motivator of partisanship and American electoral politics than previous studies might suggest.\n            \n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Can Ending At-Large Elections Encourage Racial Minorities To Run For Office? \n      \n      \n      \n      \n        \n          May 10, 2025. Zachary Lorico Hertz. Working paper.\n        \n      \n      \n      \n      \n      \n      \n      \n        \n          Abstract (click to expand)\n          \n            \n              Why do racial minorities remain underrepresented among office-holders, particularly at the local level? Previous research on descriptive representation focuses on voter choices at the ballot box and attributes the paucity of minority candidates to voter bias. At the same time, given findings that at-large elections can diminish racial minorities' political power and candidates selectively run in favorable political landscapes, institutional electoral rules might contribute to the persistent disparity in racial representation among candidates. Despite these expectations, the dynamics under which electoral institutions shape candidate emergence among racial minorities remains understudied. I utilize the switch from at-large to by-district city council elections under the California Voting Rights Act of 2001 to causally identify how and under what conditions switching to by-district elections encourages racial minorities to run for local office. The evidence suggests that ending at-large elections under the CVRA increased both the number and percent share of Latino candidates among city council candidates, and boosted the rates at which Latino candidates won city council elections. These effects are strongest in cities where Latinos are more than 20 percent of the population. I also find evidence to suggest that ending at-large elections increased candidate emergence among Asian Americans in cities where Asian Americans are more than 50 percent of the citywide population. These findings highlight the importance of candidate emergence as a crucial mechanism driving descriptive representation and suggest policy interventions targeting candidate recruitment might be as important as those addressing voter participation.\n            \n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Local Diversity and Political Participation \n      \n      \n      \n      \n        \n          May 5, 2025. Marco Mendoza Aviña and Zachary Lorico Hertz. Working paper.\n        \n      \n      \n      \n      \n      \n      \n      \n        \n          Abstract (click to expand)\n          \n            \n              Previous research finds that racial heterogeneity impacts a variety of outcomes, including participation. However, this relationship is complex and might be contingent on individual factors. This paper examines how the racial makeup of a locality influences political involvement among its residents. It pairs U.S. Census population estimates with validated voting records from the Congressional Elections Study for a quarter of a million citizens and a decade of federal elections. Racial diversity at the local level conditionally affects voters, fostering political engagement among Democrats but inhibiting it among Republicans.\n            \n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Followers or Learners? Untangling the Roles of Partisanship and Reasoning in Public Policy Preferences \n      \n      \n      \n      \n        \n          Jun 10, 2022. Zachary Lorico Hertz. Working paper.\n        \n      \n      \n      \n      \n      \n      \n      \n        \n          Abstract (click to expand)\n          \n            \n              Do people thoughtlessly support positions taken by their party leaders, or carefully alter their beliefs when given reason to do so? Many studies examine the effects of cues from party leaders on policy preferences and cast voters as party loyalists, but rarely compare information from party leaders to information from other political and nonpartisan sources and thus cannot disentangle whether people rationally update their preferences or blindly follow party leaders. To investigate, I vary cues to identify the comparative strength of party leader cues and test issue importance and previous knowledge as potential moderators. I find that when asked to support or oppose a discrete policy, partisans respond to cues from party leaders but not other cues. When respondents respond with a continuous range of policy preferences, however, party leader cues are not inherently stronger — and are sometimes weaker — than cues from other sources. I find limited evidence to suggest either issue importance or political knowledge significantly moderates partisan sensitivity to elite cues, no matter the source. These results suggest that while party leaders draw partisans to express support for individual policy planks, leaders’ influence on underlying beliefs is far more complicated and voters engage in more cognition than previously suggested.\n            \n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n\nNo matching items\n\n\n\n1"
  },
  {
    "objectID": "research/index.html#footnotes",
    "href": "research/index.html#footnotes",
    "title": "Research",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThanks to Carlisle Rainey for making a template for research pages publicly available, which I have lightly tweaked to make my own.↩︎"
  },
  {
    "objectID": "lyrics/index.html",
    "href": "lyrics/index.html",
    "title": "Zachary Lorico Hertz",
    "section": "",
    "text": "#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\n## file: app.R\n# Big Thief Lyric Bot\n\nlibrary(shiny)\nlibrary(shinythemes)\nlibrary(dplyr)\n\n# UI\nui &lt;- fluidPage(\n  theme = shinytheme(\"flatly\"),\n  \n  tags$head(\n    tags$style(HTML(\"\n      .main-container {\n        max-width: 600px;\n        margin: 20px auto;\n        text-align: center;\n      }\n      \n      .lyrics-display {\n        background: #f8f9fa;\n        border-left: 4px solid #007bff;\n        padding: 30px;\n        margin: 30px 0;\n        border-radius: 8px;\n        box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n      }\n      \n      .lyrics-text {\n        font-size: 18px;\n        line-height: 1.6;\n        color: #2c3e50;\n        font-style: italic;\n        white-space: pre-line;\n        margin-bottom: 20px;\n      }\n      \n      .attribution {\n        font-size: 14px;\n        color: #6c757d;\n        font-weight: 500;\n      }\n      \n      .btn-generate {\n        font-size: 16px;\n        padding: 12px 30px;\n        margin: 20px 0;\n        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n        border: none;\n        border-radius: 25px;\n        color: white;\n        transition: all 0.3s ease;\n      }\n      \n      .btn-generate:hover {\n        transform: translateY(-2px);\n        box-shadow: 0 5px 15px rgba(0,0,0,0.2);\n        background: linear-gradient(135deg, #764ba2 0%, #667eea 100%);\n      }\n      \n      .stats {\n        font-size: 12px;\n        color: #868e96;\n        margin-top: 20px;\n      }\n      \n      .loading {\n        color: #6c757d;\n        font-style: italic;\n      }\n    \"))\n  ),\n  \n  div(class = \"main-container\",\n    h1(\"Big Thief Lyric Generator\"),\n    p(\"This is a webapp I coded that randomly displays snippets from Big Thief's discography.\"),\n    \n    actionButton(\"generate\", \"✨ View Random Lyrics\", class = \"btn btn-generate\"),\n    \n    uiOutput(\"lyrics_display\"),\n    \n    div(class = \"stats\",\n      textOutput(\"stats_text\")\n    )\n  )\n)\n\n# Server\nserver &lt;- function(input, output, session) {\n  # Reactive value to store the loaded data\n  lyric_chunks &lt;- reactiveVal(NULL)\n  data_loaded &lt;- reactiveVal(FALSE)\n  \n  # Load data when app starts\n  observe({\n    tryCatch({\n      data_url &lt;- \"https://zacharylhertz.github.io/files/chunks.csv\"\n      temp_file &lt;- tempfile(fileext = \".csv\")\n      download.file(data_url, temp_file, mode = \"wb\")\n      chunks_data &lt;- read.csv(temp_file, stringsAsFactors = FALSE)\n      lyric_chunks(chunks_data)\n      data_loaded(TRUE)\n      unlink(temp_file)\n    }, error = function(e) {\n      showNotification(\"Failed to load lyric data. Please check your internet connection.\",\n                       type = \"error\", duration = 5)\n    })\n  })\n  \n  # Random lyric function - now uses reactive data\n  get_random_lyric &lt;- function() {\n    chunks_data &lt;- lyric_chunks()\n    if (is.null(chunks_data) || nrow(chunks_data) == 0) {\n      return(list(\n        lyrics = \"Unable to load lyrics data\",\n        attribution = \"Please refresh the page\"\n      ))\n    }\n    \n    sample_chunk &lt;- chunks_data %&gt;% slice_sample(n = 1)\n    lyrics_text &lt;- sample_chunk$lyrics_chunk\n    \n    # Create title with hyperlink if URL exists and is non-empty\n    if (!is.null(sample_chunk$url) && !is.na(sample_chunk$url) && sample_chunk$url != \"\") {\n      title_html &lt;- paste0('&lt;a href=\"', sample_chunk$url, '\" target=\"_blank\" style=\"color: inherit; text-decoration: underline;\"&gt;', \n                           sample_chunk$title, '&lt;/a&gt;')\n    } else {\n      title_html &lt;- sample_chunk$title\n    }\n    \n    attribution &lt;- paste0(\"— \", title_html, \" by \", sample_chunk$artist, \" (\", sample_chunk$year, \")\")\n    return(list(lyrics = lyrics_text, attribution = attribution))\n  }\n  \n  # Reactive values to store current lyrics\n  current_lyrics &lt;- reactiveVal(\"🎵 Click the button above to start generating lyrics.\")\n  current_attribution &lt;- reactiveVal(\"— Ready to explore Adrianne Lenker's penmanship?\")\n  \n  # Update when button is clicked\n  observeEvent(input$generate, {\n    if (data_loaded()) {\n      result &lt;- get_random_lyric()\n      current_lyrics(paste(\"🎵\", result$lyrics, \"🎵\"))\n      current_attribution(result$attribution)\n    } else {\n      showNotification(\"Data is still loading, please wait...\", type = \"warning\")\n    }\n  })\n  \n  # Render the lyrics display\n  output$lyrics_display &lt;- renderUI({\n    div(class = \"lyrics-display\",\n      div(id = \"lyrics-output\",\n        div(class = \"lyrics-text\", current_lyrics()),\n        div(class = \"attribution\", HTML(current_attribution()))\n      )\n    )\n  })\n  \n  # Stats text\n  output$stats_text &lt;- renderText({\n    if (data_loaded() && !is.null(lyric_chunks())) {\n      paste(\"Built from\", nrow(lyric_chunks()), \"lyrical chunks across Big Thief's discography.\")\n    } else {\n      \"Loading lyric data...\"\n    }\n  })\n}\n\n# Create the app\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/2024-11-05-black-nonvoters/index.html",
    "href": "posts/2024-11-05-black-nonvoters/index.html",
    "title": "What Can Survey Data Tell Us About Ideological Differences Between Black Voters and Black Nonvoters?",
    "section": "",
    "text": "I’ve made the replication code for this blog post publically available, here as a GitHub repository.\nAnalysts love the truism that elections can be seen through two lenses: persuasion and turnout. And today, as voting ends for an election that will likely be decided by a margin of a few hundred thousand voters across seven states, particular attention has been brought to the latter. A recent Good Authority piece makes the crucial point1 that Black voters are called upon to either save the day or serve as scapegoats depending on the election results, and analysis that treats Black Americans as a monolithic voting bloc misses important dynamics in the race.\nThe Good Authority piece seems, in part, to be a response to the plethora of pre-election pieces that focus on low-turnout Black voters. In particular, Nate Cohn of the New York Times has written several pieces suggesting that Black voters who did not vote in 2020 will be crucial in determining this year’s results. He’s not the only one: similar pieces ran in CNN and Time, and these claims have been made by pundits and academics alike. Yet while much ink has been spilled ahead of today’s election trying to estimate how the 2024 election might be shaped by winning over the group of Black voters who sat out 2020, very few analysts writing about Black Americans have focused on how 2020 voters and non-voters differ. Additionally, to the extent that these pieces have considered possible differences, their ability to establish definitive findings are largely limited because they are drawn from pre-election polling of relatively small subgroups, meaning large margins of error can obscure real differences.\nHow do Black Americans who chose to vote in 2020 differ from those who sat out the election? What policy areas might motivate those who sat out in 2020 to vote in 2024, and how do their attitudes on racial and gender issues differ? To answer these questions, I use the 2020 Cooperative Election Study, which presents two distinctive advantages over analysis relying on pre-election polling. The large sample size of the CES reduces the margin of error when estimating differences between Black Americans who voted in 2020 and Black Americans who stayed home in 2020 (the 2020 data has n=3,096 Black voters and n=1,816 Black non-voters); additionally, CES respondents reply to a large battery of attitudinal and policy questions, allowing me to assess differences in opinion across a wide number of potential issues. Voter status is determined using the CL_2020gvm variable: respondents with a validated voting record, no matter their mode of participation, are defined as voters. Both matched non-voters and non-matched respondents are defined as non-voters; a deeper discussion of this definition can be found in the CES Guide."
  },
  {
    "objectID": "posts/2024-11-05-black-nonvoters/index.html#footnotes",
    "href": "posts/2024-11-05-black-nonvoters/index.html#footnotes",
    "title": "What Can Survey Data Tell Us About Ideological Differences Between Black Voters and Black Nonvoters?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI highly recommend reading the piece — by Nadia E. Brown, Christopher J. Clark, Anna M. Mahoney, Periloux Peay, and Michael G. Strawbridge — in its entirety. Plus, don’t you need something to keep your mind off drawing overly strong conclusions from early returns?↩︎"
  },
  {
    "objectID": "posts/2021-07-06-ces-young-voters/index.html",
    "href": "posts/2021-07-06-ces-young-voters/index.html",
    "title": "Using the CES to Examine Young Voters",
    "section": "",
    "text": "July 1 marked 50 years since the ratification of the 26th Amendment to the United States Constitution, which legally extended the right to vote to those over the age of 18. Young voters have played a vital – at times, even decisive – role in elections since. But decades after the franchise was extended to 18-, 19-, and 20- year-olds, how does their voting rates compare to American adults overall?\nYouth have historically voted at lower rates than older voters and while some evidence suggests these turnout disparities have declined in recent elections they still remain, in part due to inadequate voter registration efforts targeted at the youngest voters and particularly high barriers to voting. As voting may be habit-forming, understanding to what extent these inequities persist among young voters is critical to rectifying them, working towards an evenly engaged electorate, and building lifelong voters.\nThe Center for Information & Research on Civic Learning and Engagement (CIRCLE) at Tufts University is one of the leading sources of research on young voters. Their research generally defines youth as those under 30, however, a broader group than those 18-20 affected by the 26th Amendment, and their data sources are unfortunately not public. Instead, I turn to the Cooperative Election Study (CES), as the premier large-scale academic survey project aimed at studying the American Electorate. The CES has over 50,000 respondents each year in its representative sample; each wave since 2010 has included over 700 respondents between ages 18 and 20, though this number has dropped in the two most recent waves.\n\nWhile the CES includes voting data matched to the Catalist LLC voter file, the 2020 validated vote data has not been released. Until then, I use the self-reported voting data from the CES to investigate, and will replicate my analysis when validated voting data from all CES waves are made publicly available. As a result, my findings come with the important caveat that self-reported measures consistently overestimate turnout.\n\nThe voter registration gap between youth and older voters remains\nWhen comparing voter registration and voter turnout, it is important to note that presidential and midterm elections should not be directly compared, as voter engagement is higher in presidential elections than midterm elections.\nCES data from the past three midterm elections (2010, 2014, and 2018) shows that the gap in voter registration numbers between the 18-20 year-old cohort and all adults has been persistent. In 2010, 81.5 percent of adults said that they were registered to vote where just 57.1 percent of 18-20 year-olds did, a difference of 24.4 percentage points. While the overall percent of adults registered to vote rose slightly to 82.2 percent in 2014, the number of youth who said they were registered to vote dropped to 51.2 percent, widening the voter registration gap to 31 percent. But in 2018 overall voter registration rose to 83.3 percent and youth voter registration rose with it to 58.8 percent, returning the voter registration gap to 24.5 percent.\n\nThe past three presidential elections (2012, 2016, and 2020) followed a similar yet opposite trend. 76 percent of all adults were registered to vote in 2012 while 57.3 percent of 18-20 year-olds were, a difference of 18.7 percentage points. In 2016 these numbers jumped noticeably: 81.6 percent of adults and 69.3 percent of 18-20 year-olds were registered to vote, closing the gap to just 12.3 percent. But in 2020, youth voter registration dropped slightly to 65.4 percent while overall voter registration rose to 84.3 percent, bringing the voter registration gap back to 18.9 percent, near its 2012 levels.\n\n\nThe turnout gap in midterm elections has decreased, but remains in presidential elections\nCES self-reported vote data shows that the voter turnout gap between 18-20 year-olds and all adults is highest in midterm elections, but has consistently decreased since 2010. 58.9 percent of all adults but just 16 percent of youth reported voting in 2010, a difference of almost 43 percentage points. Self-reported voting increased ten percentage points to 68.8 percent among all adults in 2014 while more than doubling among youth to 35 percent, decreasing the gap to 33.8 percent. By 2018, which saw 72.8 percent of adults report voting, 45 percent of youth ages 18-20 reported voting, bringing the difference in midterm elections to its lowest gap of 27.8 percentage points.\n\nIn presidential elections, the gap in self-reported voting between 18-20 year-olds and all adults is smaller, but has not followed a consistent pattern between 2012 and 2020. In 2012, 72.6 percent of adults but just 54 percent of 18-20 year-olds reported voting, a difference of 18.6 percentage points. Self-reported voting jumped in 2016: 77.4 percent of all adults and 60.3 percent of youth said they voted, decreasing the gap to just 17.1 percent. While self-reported voting increased to 78.6 percent among all adults in 2020, however, only 55.4 percent of 18-20 year-olds reported voting, deepening the turnout gap to 23.2 percentage points.\n\n\nConclusion\nOn the 50th anniversary of extending the franchise to Americans between the ages of 18 and 20, CES data shows that turnout gaps between the youngest Americans and all adults remain but may be decreasing. Gains in youth voter turnout have been especially pronounced in midterm elections, which see lower voter turnout than presidential elections, suggesting that while turnout disparities have not been completely eliminated, efforts to engage the youngest voters in the less-engaged midterm congressional elections may have been successful.\nThese findings are subject to the normal limitations of using survey data to estimate voter turnout. As relying on self-response data often overestimates turnout, I hope to replicate this analysis using the CES validated vote data once the 2020 wave is released.\n\n\n\n\nCitationBibTeX citation:@online{lorico_hertz2021,\n  author = {Lorico Hertz, Zachary},\n  title = {Using the {CES} to {Examine} {Young} {Voters}},\n  date = {2021-07-06},\n  url = {index.qmd/posts/2021-07-06-ces-young-voters/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLorico Hertz, Zachary. 2021. “Using the CES to Examine Young\nVoters.” July 6, 2021. index.qmd/posts/2021-07-06-ces-young-voters/."
  },
  {
    "objectID": "posts/2021-12-16-party-cues/index.html",
    "href": "posts/2021-12-16-party-cues/index.html",
    "title": "How Sensitive are Partisans to Out-Party Cues?",
    "section": "",
    "text": "A recent working paper by Anthony Fowler and William Howell finds that partisans update their policy beliefs in response to both in- and out-party elite cues. I found the paper especially notable for a few reasons: its findings are well-supported yet somewhat incongrous with both previous research and the oft-cited1 expression that “partisanship is a hell of a drug”.2 Given the paper’s novelty and relevance to my own research interests, I ran an extension and replication in May 2021. Do partisans update their policy beliefs in response to elite cues from both in-party and out-party leaders?\nI follow the methods outlined by Fowler and Howell to design four survey experiments. The first is a replication of the authors’ experiments on the federal minimum wage, while the other two extend their design to examine policy preferences for spending on national security and a hypothetical infrastructure bill."
  },
  {
    "objectID": "posts/2021-12-16-party-cues/index.html#footnotes",
    "href": "posts/2021-12-16-party-cues/index.html#footnotes",
    "title": "How Sensitive are Partisans to Out-Party Cues?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee, for example, its frequent use by political scientists on Twitter.↩︎\nOne which seems to have been created and popularized by Brendan Nyhan and Stephen Miller.↩︎\nThe infrastructure bill had no status-quo equivalent, so respondents were simply presented with the sentence: “Congress is currently considering an infrastructure bill to create jobs and rebuild national infrastructure.”↩︎\nWhile the analysis of pooled partisans excluding leaners is omitted for brevity, doing so actually strengthens the treatment effects slightly to 0.25 for both in-party and out-party positions in the minimum wage experiment.↩︎\n[Photo by Kelly Sikkema on Unsplash.]↩︎"
  },
  {
    "objectID": "posts/2021-06-29-survey-package/index.html",
    "href": "posts/2021-06-29-survey-package/index.html",
    "title": "An Introduction to Using the {survey} Package in R",
    "section": "",
    "text": "Survey research commonly relies on weights to reduce bias and produce a representative sample for a given population of interest. Weighted survey data produces a value assigned to each observation in the data that increases or decreases that observation’s influence (or weight) when performing statistical operations using the data.\nCorrectly implementing weights can seem an intimidating challenge to early R users; luckily several packages exist to simplify working with weighted data in R. The package I currently use is {survey}, which I have used to produce several pieces in my work for Data for Progress and the Tufts Public Opinion Lab.\nTo that end, I have written a quick guide to using the {survey} package in R to create weighted proportion tables and plot results using {ggplot2}. This primer uses the Data for Progress Covid-19 tracking poll data and assumes an elementary knowledge of coding in R. This guide was originally written for one of my Tufts Public Opinion Lab colleagues as well as my Political Science Research Methods students, but I hope others benefit from it!\n\nSetup\nTo start, you’ll need to read in the necessary packages and then the data. Beyond {survey} for weighted analysis and {tidyverse} to use ggplot2 to visualize results, I use a few additional packages: {haven}, {magrittr}, and {plyr}. Since my data is from a .dta file, I use {haven} to read the data into R. I also use the function multiply_by() and the pipe operator, %&gt;%, from the {magrittr} package. Finally, when manipulating my plot data I rely on the ddply() function from {plyr}.\nlibrary(haven)\nlibrary(survey)\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(plyr)\ndat &lt;- read_dta(\"dfp_covid_tracking_poll.dta\")\nWeights are generally stored in a column within your dataframe, and may differ depending on which population you are attempting to represent with your data. You should be able to identify the weights and their appropriate variable name using the codebook. Here, we weight to the American adult populations using the variable nationalweight.\nTo use the weights, you first create a survey design object using the svydesign() function from {survey}, and specify the appropriate dataframe and weights.\n# First, create a survey design object for the dataset.\n# Here, my data is 'dat' and weights are 'nationalweight'.\nsvy.dat &lt;- svydesign(ids=~1, data=dat,\n                     weights=dat$nationalweight)\nYou can also create a survey design object using subsets, which can be particularly useful to analyze specific parts of your data. To do so, simply specify the subset instead of your full data in the ‘data’ argument of svydesign().\nTo illustrate, I will compare vaccination likelihood between White Evangelicals and the sample as a whole. Race is represented by the ethnicity variable, where White respondents have a value of 1, and evangelicalism is represented by the pew_bornagain variable, where Evangelicals have a value of 1.\n# Create your white evangelical subset...\nwtevan &lt;- subset(dat, dat$ethnicity==1&dat$pew_bornagain==1)\n\n# ...And create a survey design object for White Evangelicals.\nsvy.evan &lt;- svydesign(ids = ~1, data = wtevan,\n                      weights = wtevan$nationalweight)\nNote that above you use the same weight variable from your full data, in this case nationalweight, but in the weights = argument of svydesign() you have to pull the weight variable from the same dataframe you use in the data = argument.\n\n\nUsing {survey} to create weighted proportion tables\nNow that we have survey design objects, we use them in combination with the svytable() function to apply the weights. The syntax is very intuitive, essentially:\nsvytable(~Var1 + Var2 + ..., design=your.surveydesignobject)\nSimply include which variables you would like to table, separating variables with ‘+’, and specify the appropriate survey design object. I then use prop.table() to convert counts to a proportion, multiply by 100 to create total percentages, round to one digit, and convert the result to an easily-viewable dataframe.\nAs a quick example using multiple variables, I create a table of vaccine likelihood (vax_likely), education (educ), and party identification (pid7) with the following code:\nvax.educ.race &lt;- svytable(~vax_likely + educ + pid7, \n                     design=svy.dat) %&gt;%\n  prop.table() %&gt;%\n  multiply_by(100) %&gt;%  \n# Note that multiply_by() comes from magrittr!\n  round(digits=1) %&gt;%\n  as.data.frame()\n\nhead(vax.educ.race)\n##   vax_likely educ pid7 Freq\n## 1          1    1    1  0.6\n## 2          2    1    1  0.1\n## 3          3    1    1  0.3\n## 4          4    1    1  0.1\n## 5          1    2    1  3.1\n## 6          2    2    1  1.7\nApplying this to our investigation of vaccine likelihood for White Evangelicals compared to national adults, we can create two tables. The first uses the survey design object created with the entire dataset (svy.dat), the other uses the survey design object created for just White Evangelicals (svy.evan).\n# Vaccination likelihood among all American adults\nlikely &lt;- svytable(~vax_likely, design=svy.dat) %&gt;%\n  prop.table() %&gt;%\n  multiply_by(100) %&gt;%\n  round(digits=1) %&gt;%\n  as.data.frame()\n\n# Vaccination likelihood among White Evangelicals  \nlikely.evan &lt;- svytable(~vax_likely, design=svy.evan) %&gt;%\n  prop.table() %&gt;%\n  multiply_by(100) %&gt;%\n  round(digits=1) %&gt;%\n  as.data.frame()\n  \n# Vax likelihood among all American adults\nlikely \n##   vax_likely Freq\n## 1          1 34.7\n## 2          2 22.6\n## 3          3 14.5\n## 4          4 28.2\n# Vax likelihood among White Evangelical adults\nlikely.evan \n##   vax_likely Freq\n## 1          1 29.7\n## 2          2 25.2\n## 3          3 14.6\n## 4          4 30.5\n\n\nPlotting weighted results with {survey} and {ggplot2}\nTo visualize my results, I create a variable labelling each group, then combine the two tables into a single dataframe. I also use ddply() to calculate the halfway point of each proportion and use that position to specify where labels should go on the final plot.\n# Add variable identifying each group\nlikely$group &lt;- \"All respondents\"\nlikely.evan$group &lt;- \"White Evangelicals\"\n\n# Combine into a single dataframe for plotting\nplot &lt;- rbind(likely, likely.evan)\n\n# Create a variable, label_ypos, equal to the halfway point\n# in each bar for specifying labels.\nplot &lt;- ddply(plot, \"group\",\n                   transform, \n                   label_ypos=cumsum(Freq) - 0.5*Freq)\nHaving created a single dataframe, I use ggplot2 as usual to plot my results.\np &lt;- ggplot(plot, \n            aes(x = group, y = Freq, fill = vax_likely)) + \n  geom_bar(stat=\"identity\", \n           position = position_stack(reverse = TRUE)) + \n  coord_flip() + theme_minimal() + \n  theme(plot.title = element_text(hjust = 0.5, \n                                  size = 16), \n        plot.caption = element_text(hjust = 1,\n                                    face = \"italic\", size=8),\n        axis.title.x = element_text(size=10), \n        axis.title.y = element_blank(),\n        axis.text.y = element_text(size=10, \n                                   color = \"black\")) + \n  ylab(\"Percent\") + \n  labs(fill=\"Likeliness to get Covid-19 \n  vaccine when available\", \n       caption = \"Zachary L. Hertz | Data: Data For Progress\") +\n  ggtitle(\"White Evangelicals remain skeptical of Covid-19 vaccine\") +\n  scale_fill_manual(labels = c(\"Very likely\", \n                               \"Somewhat likely\", \n                             \"Somewhat unlikely\", \n                             \"Very unlikely\"), \n                    values=c(\"#7aa457\", \n                             \"#b6caa2\", \n                             \"#e4ac9e\", \n                             \"#cb6751\")) +\n  geom_text(aes(y=label_ypos, label=Freq), \n            color=\"white\", size=3.5)\np\n\n\n\nConclusion\nThe {survey} package is a flexible and powerful tool to use weights in survey analysis and visualization. Using the svydesign() and svytable() functions is an easy way to create weighted proportion tables and examine representative data. As I often rely upon public tutorials when learning new skills in R, I hope this guide proves useful to any future researchers hoping to grow their survey analysis skills using R!\n\n\n\n\nCitationBibTeX citation:@online{lorico_hertz2021,\n  author = {Lorico Hertz, Zachary},\n  title = {An {Introduction} to {Using} the `\\{Survey\\}` {Package} in\n    {R}},\n  date = {2021-06-29},\n  url = {index.qmd/posts/2021-06-29-survey-package/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLorico Hertz, Zachary. 2021. “An Introduction to Using the\n`{Survey}` Package in R.” June 29, 2021. index.qmd/posts/2021-06-29-survey-package/."
  },
  {
    "objectID": "posts/2021-06-22-dfp-lucid/index.html",
    "href": "posts/2021-06-22-dfp-lucid/index.html",
    "title": "Investigating Concerns about Lucid Theorem Data Quality",
    "section": "",
    "text": "The blossoming use of survey research in political science heightens the need for rigorous investigation into data quality. To obtain samples, academics often rely on survey vendors such as SurveyMonkey, Amazon’s Mechanical Turk (MTurk), and Lucid Theorem. Use of data from Lucid has become increasingly prevalent, and the focus of growing data quality concerns.\nMore recently, Josh Kalla posted a thread on Twitter noting that when running a near-replication of a survey from March 2021 in June 2021, the median response time had dropped over five minutes. This result is particularly troubling, as a dramatic decrease in median response time could indicate extensive satisficing by respondents, colloquially referred to as “speeders,” and potentially induce bias into the results.\nStill, while these findings certainly raise cause for concern, any response time issues between these particular two surveys may reflect nuanced differences in the question battery or result from pure chance. A survey with a static battery of questions fielded consistently and repeatedly via Lucid over a wide time period is the ideal way to clarify whether there have been recent and notable decreases in response time among Lucid’s respondents.\nFortunately, I have access to such a survey: the Data For Progress Covid-19 tracking poll, fielded via Lucid by Brian Schaffner. There have been 25 waves of the poll between April 2020 and May 2021, and besides small modifications has remained almost identical over that time period. I used data from the tracking poll to investigate whether our data from Lucid saw any changes in response time over the fourteen-month period, and check whether the Lucid-provided partisan measure correlated strongly with the ideology question included in the poll.\n\nMedian response time was generally stable\nThe median response time was generally stable until October, when it dropped through the fall and winter, returning to previous levels in the spring. Between April 2020 and May 2021, the median response time to the DFP Covid-19 tracking poll ranged from 12.35 to 14.43 minutes. In the poll’s first eight iterations, response times mostly increased, reaching the poll’s greatest median response time during Wave Eight on June 09, 2020. Over the next eight waves, between June 23 and September 22, the median response time was fairly static and stayed between 13 and 14 minutes. Response times dropped to twelve minutes in October and stayed there over the next six waves into February 2021. The February 09, 2021 wave saw a slight increase in median response time, and the March, April, and May waves all had median response times near the previous mid 13-minute mark. For the most part, the first and third quartiles followed similar trends, suggesting that there were no dramatic changes in the distribution of response times among our Lucid Theorem respondents.\n\nStill, while tracking the median response time over 13 months is a convenient measure to identify whether the proportion of speeders among Lucid respondents have been increasing, we might prefer to examine the full distribution of response times over all 25 waves of the Covid-19 tracking poll to glean more information. I visualized this distribution for each wave below using the {gganimate} package. Plotting the median, as well as the first and third quartiles, helps identify any notable shifts in the distribution of response times.\n\n\n\nUsing correlations to check data quality\nWe can also attempt to measure data quality by testing whether variables that should be correlated based on theory actually are correlated in our data. Lucid provides respondent demographics across several variables, including age, education, ethnicity, gender, household income, party identification and region. I also investigated whether the respondent party identification information provided by Lucid is correlated as expected with respondent-provided political ideology. To provide a baseline, I coded identical political party and political ideology using the 2020 CES, which had a correlation coefficient of 0.668.\nWe see that the correlation between party identification and ideology was higher than 0.5 in a majority of waves, but remains lower than expected from the CES data. There were two notable shifts. The correlation fell, rose, and then dropped again over the five waves fielded between May 5, 2020 and June 9, 2020. There was also a sharp drop in the correlation between Lucid-provided party identification and respondent ideology from 0.594 on September 22, 2020 to 0.391 on October 6, 2020. The correlation has risen steadily since then, and was 0.624 for the wave fielded on May 15, 2021.\n\n\n\nConclusion\nOverall, as the median and quartile measures for response times remain fairly stable over time and follow similar trends, I find limited evidence that the proportion of speeders have recently been increasing among respondents recruited via Lucid Theorem. Additionally, research finds that speeders have a limited effect on data quality, potentially assuaging concerns about external validity even if speeders were becoming more common among data gathered using Lucid Theorem.\nStill, these findings follow important caveats. The data from the DFP tracking poll includes respondents who pass attention checks. Yet an increasing number of survey respondents are failing attention checks, and those who do fail are markedly different from attentive respondents. Speeders may be increasing among the population of Lucid respondents but end up filtered from the data after failing attention checks, and attention checks remain critical to maintaining data quality. To guard against inattentive respondents and continue to monitor for potential declines in response quality among survey vendors, researchers must continue to include attention checks while building in questions and methods to independently audit their data.\n\n\n\n\nCitationBibTeX citation:@online{lorico_hertz2021,\n  author = {Lorico Hertz, Zachary},\n  title = {Investigating {Concerns} about {Lucid} {Theorem} {Data}\n    {Quality}},\n  date = {2021-06-22},\n  url = {index.qmd/posts/2021-06-22-dfp-lucid/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLorico Hertz, Zachary. 2021. “Investigating Concerns about Lucid\nTheorem Data Quality.” June 22, 2021. index.qmd/posts/2021-06-22-dfp-lucid/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zachary Lorico Hertz",
    "section": "",
    "text": "Welcome to my website!\nI am a researcher, political consultant, and data journalist driven by an interest in communities.  In my research, I use geocoded observational data and original survey data to understand questions at the intersection of political behavior, identity politics, local politics, and American political institutions.  I love my friends, talking about political problems, and being outside. Sometimes all at once!\nLearn more about me here, check out my research, or say hi!"
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Use the filters on the right to browse courses I have taught by topic."
  },
  {
    "objectID": "teaching/index.html#university-of-california-berkeley",
    "href": "teaching/index.html#university-of-california-berkeley",
    "title": "Teaching",
    "section": "University of California, Berkeley",
    "text": "University of California, Berkeley\n\n\n\n\n\n  \n    \n      PS 3: Introduction to Empirical Analysis and Quantitative Methods \n      \n      \n      \n      \n        Fall 2025. Graduate student instructor for David Broockman.\n      \n      \n      \n      \n\n      \n      \n        \n          Course Description (click to expand)\n          \n            \n              Data is increasingly central to politics, policy, law, business, and life. No matter where your life or career takes you, you will need to rely on analyzing and interpreting data -- be it understanding how persuasive a study summarized in a news article is, making a case before a judge, deciding which voters to target in your political campaign, as a political leader trying to decide what policy best serves your constituents, or as a political scientist trying to understand how politics works. The purpose of this class is to equip you to better understand how to gather and interpret data that speaks to important questions of all kinds. Much of the course, although not all of it, will focus on applications in political science, teaching you how to pose and answer political science research questions in a rigorous way.\n            \n          \n        \n      \n\n      \n      \n\n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n      \n      \n    \n  \n    \n      PS 109-B: The Politics of Public Policy \n      \n      \n      \n      \n        Summer 2025. Graduate student instructor for David Broockman.\n      \n      \n      \n      \n\n      \n      \n        \n          Course Description (click to expand)\n          \n            \n              Government policy deeply influences every aspect of our lives, including the quality of the universities we attend, how much it costs for a gallon of gas, what training we must obtain to take what jobs, what technologies are available to us to use, who we can marry, whether we can afford critical necessities such as housing, and what medical treatments we are allowed or not allowed to obtain. &lt;br&gt; Why do governments make the policies they do? How can advocates craft effective political strategies to influence government policy? &lt;br&gt; This course focuses on understanding the political forces that shape public policy, including interest groups, public pressure, political parties, and voters. In contrast to a public policy course, this course will not consider what ideal public policies should be, but rather consider why governments make the policies they do. &lt;br&gt; This course is suitable for anyone interested in politics or policy. Students will learn tools that will allow them to think like political strategists and understand how they and others influence government in practice.\n            \n          \n        \n      \n\n      \n      \n        \n           Syllabus\n         \n      \n\n      \n      \n        \n           Section Syllabus\n         \n      \n      \n      \n      \n\n      \n      \n      \n      \n      \n      \n    \n  \n    \n      PS 109-E: The US Executive Branch and its Political Environment \n      \n      \n      \n      \n        Spring 2025. Graduate student instructor for Sean Gailmard.\n      \n      \n      \n      \n\n      \n      \n        \n          Course Description (click to expand)\n          \n            \n              This course is about how the US government gets things done: the executive branch. Much of what the law actually says is decided in the executive branch, and almost all of it it implemented there. This involves the bureaucracy as well as (often more than) the president. Because implementation of policy defines what policy is, other parts of the government try to influence what the executive branch does, and different parts of the executive branch try to influence each other. &lt;br&gt; The course is structured around three major components. First, we will look at the executive branch with a focus on the president. We will analyze the president’s incentives and constraints in controlling the executive branch, and how presidents’ responses have changed American politics. Second, we will examine the executive branch with a focus on the bureaucracy. We will examine the paths by which bureaucrats make policy, and the ways that other actors try to influence them. Third, we will focus on two case studies to examine the ideas we develop in practice: one on national security and defense policy, one on immigration policy and enforcement.\n            \n          \n        \n      \n\n      \n      \n        \n           Syllabus\n         \n      \n\n      \n      \n        \n           Section Syllabus\n         \n      \n      \n      \n      \n\n      \n      \n      \n      \n      \n      \n    \n  \n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html#tufts-university",
    "href": "teaching/index.html#tufts-university",
    "title": "Teaching",
    "section": "Tufts University",
    "text": "Tufts University\n\n\n\n\n\n  \n    \n      PS 103: Political Science Research Methods \n      \n      \n      \n      \n        Spring 2021. Teaching assistant for Brian Schaffner.\n      \n      \n      \n      \n\n      \n      \n        \n          Course Description (click to expand)\n          \n            \n              Political scientists frequently use quantitative methods to address questions about citizens’ polit- ical attitudes, elections, wars, policy outcomes, and other important political phenomena. This course will consider the general concepts underlying empirical research, including causal inference, research design, statistical analysis, and programming. The goal is to help students become in- formed consumers of quantitative social science research and provide them with useful tools for undertaking empirical research of their own.\n            \n          \n        \n      \n\n      \n      \n        \n           Syllabus\n         \n      \n\n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n      \n      \n    \n  \n\nNo matching items"
  }
]